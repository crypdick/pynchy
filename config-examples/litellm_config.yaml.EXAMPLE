# LiteLLM Proxy Configuration — EXAMPLE
# =======================================
# NOTE: This is a reference example checked into the repo. The live config
# is on pynchy-server (reachable via Tailscale):
#   ssh pynchy-server cat ~/src/PERSONAL/pynchy/litellm_config.yaml
#
# Copy to litellm_config.yaml (project root) and fill in your API keys.
# This file is the single source of truth for LLM routing — pynchy reads
# it but never modifies it.
#
# How env vars work:
#   1. Reference API keys with os.environ/VARNAME in this file.
#   2. Set the matching vars in .env (e.g. ANTHROPIC_TOKEN_EMPLOYEE1=sk-ant-...).
#   3. At startup pynchy scans this file, finds all os.environ/ references,
#      looks them up on the host, and forwards them into the Docker container.
#
# Key safety: pynchy filters model_list entries at startup. If an api_key's
# env var is unset or contains a placeholder value (e.g. "sk-ant-..."),
# that deployment is silently removed from the config mounted into LiteLLM.
# This prevents zombie deployments from poisoning the router's health state.
#
# To add a second key for load balancing, just add another model_list entry
# with a different os.environ/ var and set that var in .env — no code changes.
#
# Docs: https://docs.litellm.ai/docs/proxy/configs
# Wildcard routing: https://docs.litellm.ai/docs/wildcard_routing
#
# To enable: set gateway.litellm_config = "litellm_config.yaml" in config.toml

# ---------------------------------------------------------------------------
# Model deployments
# ---------------------------------------------------------------------------
# Wildcard routing proxies all models from a provider without listing each
# one individually. When you change the model in the Claude SDK config,
# it just works — no config update needed here.
#
# Multiple entries with the same model_name are load-balanced automatically.

model_list:
  # --- Anthropic (primary account) ---
  - model_name: "anthropic/*"
    litellm_params:
      model: "anthropic/*"
      api_key: os.environ/ANTHROPIC_TOKEN_EMPLOYEE1
      budget_duration: 7d
      custom_llm_provider: anthropic
    model_info:
      id: anthropic-employee1
      tags: ["mycompany"]

  # --- Anthropic (secondary account, for load balancing / failover) ---
  # Set ANTHROPIC_TOKEN_EMPLOYEE2 in .env to enable.  If the env var is
  # unset or contains a placeholder, pynchy automatically removes this
  # entry before starting LiteLLM.
  - model_name: "anthropic/*"
    litellm_params:
      model: "anthropic/*"
      api_key: os.environ/ANTHROPIC_TOKEN_EMPLOYEE2
      max_budget: 50.0  # USD
      budget_duration: 7d
      custom_llm_provider: anthropic
    model_info:
      id: anthropic-employee2
      tags: ["mycompany"]

  # --- Anthropic (OAuth token from `claude setup-token`) ---
  # OAuth tokens (sk-ant-oat01-...) work as api_key values — LiteLLM
  # auto-detects the prefix and adds the required beta header.
  # Set CLAUDE_OAUTH_TOKEN in .env to enable.
  # - model_name: "anthropic/*"
  #   litellm_params:
  #     model: "anthropic/*"
  #     api_key: os.environ/CLAUDE_OAUTH_TOKEN
  #     custom_llm_provider: anthropic
  #   model_info:
  #     id: anthropic-oauth
  #     tags: ["mycompany"]

  # --- OpenAI ---
  # - model_name: "openai/*"
  #   litellm_params:
  #     model: "openai/*"
  #     api_key: os.environ/OPENAI_KEY_EMPLOYEE1
  #     budget_duration: 7d
  #     custom_llm_provider: openai
  #   model_info:
  #     id: openai-employee1
  #     tags: ["mycompany"]

# ---------------------------------------------------------------------------
# Router settings  —  failover tuning
# ---------------------------------------------------------------------------
# See the litellm-diagnostics reference for detailed rationale.
router_settings:
  # How to distribute requests across deployments with the same model_name.
  # Options: simple-shuffle, least-busy, usage-based-routing,
  #          latency-based-routing, cost-based-routing
  routing_strategy: usage-based-routing

  # Retry failed requests on a different deployment
  num_retries: 3
  timeout: 300

  # Cooldown: after (allowed_fails + 1) consecutive failures on a
  # deployment, take it out of rotation for cooldown_time seconds.
  #
  # The check is "fails > allowed_fails", so allowed_fails=1 means the
  # deployment is cooled down after the 2nd failure.  This tolerates one
  # transient error (e.g. a brief rate limit) but quickly removes a dead
  # key from rotation.
  #
  # WARNING: Do NOT set allowed_fails to 0.  LiteLLM runs startup health
  # probes that can fail transiently; allowed_fails=0 causes those probe
  # failures to immediately cool down ALL deployments, leaving zero
  # healthy endpoints.
  allowed_fails: 1

  # 10 min — long enough to avoid per-request churn on an exhausted key,
  # short enough to recover when the quota resets.
  cooldown_time: 600

  # Provider-level budget caps (combined across all keys for that provider).
  # Requires the PostgreSQL sidecar (started automatically by pynchy).
  # For per-key budgets, use max_budget in litellm_params above.
  #
  # provider_budget_config:
  #   anthropic:
  #     budget_limit: 200.0   # USD
  #     time_period: 30d
  #   openai:
  #     budget_limit: 50.0
  #     time_period: 30d

# ---------------------------------------------------------------------------
# General settings
# ---------------------------------------------------------------------------
general_settings:
  # Master key is set in config.toml [gateway].master_key and passed via env var.
  # Do NOT hardcode it here.
  master_key: os.environ/LITELLM_MASTER_KEY

  # Max parallel requests across all models
  global_max_parallel_requests: 50

  # Public routes: Make routes accessible without auth
  # Only use this if LiteLLM is behind a secure network (e.g., Tailnet, VPN).
  # This allows the UI dashboard and API to be accessed without auth tokens.
  #
  # public_routes:
  #   - "/*"  # All routes public (only use on private networks!)

  # UI credentials are set in .env (GATEWAY__UI_PASSWORD) and config.toml
  # (gateway.ui_username). Pynchy passes them to LiteLLM as UI_USERNAME
  # and UI_PASSWORD env vars.

# ---------------------------------------------------------------------------
# MCP gateway
# ---------------------------------------------------------------------------
# LiteLLM acts as the MCP gateway when mcp_servers are defined in config.toml.
# Pynchy registers MCP endpoints and creates per-workspace teams via the
# LiteLLM HTTP API at boot. No additional configuration needed here —
# just ensure LiteLLM supports MCP routing (v1.55+).
