# LiteLLM Proxy Configuration
# ============================
# Copy to litellm_config.yaml (project root) and fill in your API keys.
# This file is the single source of truth for LLM routing — pynchy reads
# it but never modifies it.
#
# How env vars work:
#   1. Reference API keys with os.environ/VARNAME in this file.
#   2. Set the matching vars in .env (e.g. PYNCHY_ANTHROPIC_TOKEN=sk-ant-...).
#   3. At startup pynchy scans this file, finds all os.environ/ references,
#      looks them up on the host, and forwards them into the Docker container.
#
# To add a second key for load balancing, just add another model_list entry
# with a different os.environ/ var and set that var in .env — no code changes.
#
# Docs: https://docs.litellm.ai/docs/proxy/configs
# Wildcard routing: https://docs.litellm.ai/docs/wildcard_routing
#
# To enable: set gateway.litellm_config = "litellm_config.yaml" in config.toml

# ---------------------------------------------------------------------------
# Model deployments
# ---------------------------------------------------------------------------
# Wildcard routing proxies all models from a provider without listing each
# one individually. When you change the model in the Claude SDK config,
# it just works — no config update needed here.
#
# Multiple entries with the same model_name are load-balanced automatically.

model_list:
  # --- Anthropic (auto-discovered token) ---
  - model_name: "anthropic/*"
    litellm_params:
      model: "anthropic/*"
      api_key: os.environ/PYNCHY_ANTHROPIC_TOKEN
      budget_duration: 7d
      custom_llm_provider: anthropic
    model_info:
      id: anthropic-primary
      tags: ["primary"]

  # --- Anthropic (secondary account, for load balancing) ---
  # Set PYNCHY_ANTHROPIC_TOKEN_SECONDARY in .env to enable:
  - model_name: "anthropic/*"
    litellm_params:
      model: "anthropic/*"
      api_key: os.environ/PYNCHY_ANTHROPIC_TOKEN_SECONDARY
      max_budget: 50.0  # USD
      budget_duration: 7d
      custom_llm_provider: anthropic
    model_info:
      id: anthropic-secondary
      tags: ["secondary"]

  # --- OpenAI ---
  # - model_name: "openai/*"
  #   litellm_params:
  #     model: "openai/*"
  #     api_key: os.environ/OPENAI_API_KEY
  #     budget_duration: 7d
  #     custom_llm_provider: openai
  #   model_info:
  #     id: openai-primary
  #     tags: ["primary"]

# ---------------------------------------------------------------------------
# Router settings
# ---------------------------------------------------------------------------
router_settings:
  # How to distribute requests across deployments with the same model_name.
  # Options: simple-shuffle, least-busy, usage-based-routing,
  #          latency-based-routing, cost-based-routing
  routing_strategy: usage-based-routing

  # Retry failed requests on a different deployment
  num_retries: 2
  timeout: 300

  # Cool down a deployment for 60s after 3 consecutive failures
  allowed_fails: 3
  cooldown_time: 60

  # Provider-level budget caps (combined across all keys for that provider).
  # Requires the PostgreSQL sidecar (started automatically by pynchy).
  # For per-key budgets, use max_budget in litellm_params above.
  #
  # provider_budget_config:
  #   anthropic:
  #     budget_limit: 200.0   # USD
  #     time_period: 30d
  #   openai:
  #     budget_limit: 50.0
  #     time_period: 30d

# ---------------------------------------------------------------------------
# General settings
# ---------------------------------------------------------------------------
general_settings:
  # Master key is set in config.toml [gateway].master_key and passed via env var.
  # Do NOT hardcode it here.
  master_key: os.environ/LITELLM_MASTER_KEY

  # Max parallel requests across all models
  global_max_parallel_requests: 50

  # Public routes: Make routes accessible without auth
  # Only use this if LiteLLM is behind a secure network (e.g., Tailnet, VPN).
  # This allows the UI dashboard and API to be accessed without auth tokens.
  #
  # public_routes:
  #   - "/*"  # All routes public (only use on private networks!)

  # UI credentials are set in .env (GATEWAY__UI_PASSWORD) and config.toml
  # (gateway.ui_username). Pynchy passes them to LiteLLM as UI_USERNAME
  # and UI_PASSWORD env vars.

# ---------------------------------------------------------------------------
# MCP gateway
# ---------------------------------------------------------------------------
# LiteLLM acts as the MCP gateway when mcp_servers are defined in config.toml.
# Pynchy registers MCP endpoints and creates per-workspace teams via the
# LiteLLM HTTP API at boot. No additional configuration needed here —
# just ensure LiteLLM supports MCP routing (v1.55+).
